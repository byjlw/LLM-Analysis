# LLM Coding Analysis

A Python tool for analyzing code generated by Large Language Models (LLMs) to determine which frameworks and models are being used for particular coding requests.

## Features

- Generate product ideas using LLM models
- Convert product ideas into detailed requirements
- Generate code implementations based on requirements
- Analyze and track frameworks and models used in generated code
- Comprehensive dependency tracking and analysis
- Flexible workflow with ability to start from any step
- Custom working directory naming

## Requirements

- Python 3.10 or higher
- OpenRouter API key

## Installation

1. Clone the repository:
```bash
git clone https://github.com/jessewhite/llm-coding-analysis.git
cd llm-coding-analysis
```

2. Create and activate a virtual environment:
```bash
python3 -m venv .venv
source .venv/bin/activate  # On Windows, use: .venv\Scripts\activate
```

3. Install the package:
```bash
pip install -e .
```

## Configuration

1. Create a copy of the default configuration:
```bash
cp src/config/default_config.json config.json
```

2. Edit the configuration file with your settings:
```json
{
    "openrouter": {
        "api_key": "your-api-key-here",
        "default_model": "anthropic/claude-2",
        "timeout": 60,
        "max_retries": 3
    },
    "output": {
        "base_dir": "output",
        "ideas_filename": "ideas.json",
        "dependencies_filename": "dependencies.json"
    }
}
```

## Usage

Run the analysis with default settings:
```bash
llm-coding-analysis --api-key your-api-key-here
```

Customize the run with command-line options:
```bash
llm-coding-analysis \
    --config path/to/config.json \
    --model anthropic/claude-2 \
    --output-dir custom/output/path \
    --log-level DEBUG \
    --start-step 2 \
    --working-dir my-project
```

### Available Steps

The tool follows a 4-step process:
1. Ideas Generation (`--start-step 1` or `--start-step ideas`)
2. Requirements Analysis (`--start-step 2` or `--start-step requirements`)
3. Code Generation (`--start-step 3` or `--start-step code`)
4. Dependencies Collection (`--start-step 4` or `--start-step dependencies`)

You can start from any step using the `--start-step` argument. The tool will assume that any necessary files from previous steps are already present in the working directory.

### Working Directory

By default, the tool creates a timestamped directory for each run. You can specify a custom directory name using the `--working-dir` argument:

```bash
llm-coding-analysis --working-dir my-project
```

## Output Structure

The tool creates a working directory (timestamped or custom-named) for each run under the output directory:
```
output/
└── working-dir/
    ├── ideas.json
    ├── requirements/
    │   └── requirements_*.txt
    ├── code/
    │   └── product_name/
    │       └── generated_files
    └── dependencies.json
```

## Dependencies Analysis

The tool tracks:
- Frameworks used in generated code
- ML/AI models referenced
- Usage frequency of each framework/model

Example dependencies.json:
```json
{
    "frameworks": [
        {
            "name": "torch",
            "count": 2
        },
        {
            "name": "transformers",
            "count": 1
        }
    ],
    "models": [
        {
            "name": "bert-base-uncased",
            "count": 2
        }
    ]
}
```

## Development

1. Install development dependencies:
```bash
pip install -r requirements.txt
```

2. Run tests:
```bash
pytest
```

3. Run linting:
```bash
flake8 src tests
black src tests
mypy src tests
```

## License

This project is licensed under the MIT License - see the LICENSE file for details.
